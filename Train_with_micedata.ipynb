{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb3da27-83b6-4619-98f6-a024f16cadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ripon\\Anaconda3\\envs\\MLDL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bd436-cf7e-49be-90a5-d752309c2198",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('./EPM_12_image/2610.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d1e04-8bea-4973-b899-07e1d79b0d4d",
   "metadata": {},
   "source": [
    "img3 = np.asarray(img)[150:850, 250:950]\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8618767-a9d9-47e9-ba99-41ac32b29270",
   "metadata": {},
   "source": [
    "img3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928ba60-fb4e-4d21-981d-49f68bd2b9bf",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8aa66de-e99b-404f-8cb7-fb8574a11a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['frame', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# need to crop all the images to get only the mouse image\n",
    "# rading xlxs files for the images and labels\n",
    "import pandas as pd\n",
    "df = pd.read_excel('./EMP_12xlsx.xlsx')\n",
    "print(df.columns)\n",
    "#df['frame'], df['label']\n",
    "#dropping null values\n",
    "df = df.dropna()\n",
    "images = [f'./EPM_12_image/{int(i)}.png' for i in df['frame']]\n",
    "labels = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf7075-b359-4480-905f-74256981867c",
   "metadata": {},
   "source": [
    "img = Image.open(images[1])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406603c2-d452-45f1-8d49-af43c0376949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e027d19d-2a60-4571-ad3f-fd280e2d16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are seperate transfromation func for PIL and Tensor. SO chcek again if you are using the correct ones\n",
    "def get_train_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1, 5)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomRotation(degrees=(0,100)),\n",
    "    #transforms.RandomInvert(),\n",
    "    #transforms.RandomAdjustSharpness(sharpness_factor=2),isnot a good choice for the dataste\n",
    "    #transforms.RandomAutocontrast(), isnot a good choice for the dataste\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "def get_test_transform():\n",
    "    return torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff0340b-723b-44f2-b91a-53e143eee166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d539ae-c450-46e3-b834-46d9f226d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, \n",
    "                                             test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f9ff69-6cf9-4cfc-a62e-fc13bf811142",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [i for i in y_train]\n",
    "y_test = [i for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029dee58-89af-4288-b6b2-624a18550957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 90, 210, 90)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f61cd1-3c23-46dc-ba67-c9612f4e6fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['heading up', 'heading up and rearing', 'moving alone', 'rearing',\n",
       "        'stopeed alone', 'stopeed huddled', 'stoping alone',\n",
       "        'stopped alone', 'stopped huddled', 'stretched attennd posture'],\n",
       "       dtype='<U25'),\n",
       " 10,\n",
       " array(['heading up', 'heading up and rearing', 'moving alone',\n",
       "        'stopeed huddled', 'stopped alone', 'stopped huddled',\n",
       "        'stretched attennd posture'], dtype='<U25'),\n",
       " 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train),len(np.unique(y_train)), np.unique(y_test), len(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b98e378-d222-4d8b-9de3-4cd46908a0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['moving alone', 'stoping alone', 'heading up', 'rearing',\n",
       "        'stopped huddled', 'heading up and rearing', 'stopped alone',\n",
       "        'stopeed huddled', 'stretched attennd posture', 'stopeed alone'],\n",
       "       dtype=object),\n",
       " 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique(), len(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e756cee7-48a0-4410-bfe9-ba71cba5d244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109, 1, 45, 9, 53, 44, 27, 3, 8, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting data for each label\n",
    "labels_count = [len(labels[labels==i]) for i in labels.unique()]\n",
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735d12cd-546c-4acb-ac0b-0add870a4af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moving alone': 0,\n",
       " 'stoping alone': 1,\n",
       " 'heading up': 2,\n",
       " 'rearing': 3,\n",
       " 'stopped huddled': 4,\n",
       " 'heading up and rearing': 5,\n",
       " 'stopped alone': 6,\n",
       " 'stopeed huddled': 7,\n",
       " 'stretched attennd posture': 8,\n",
       " 'stopeed alone': 9}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assigning class index to every class\n",
    "classes = labels.unique()\n",
    "\n",
    "for cls in classes:\n",
    "    class_to_int = {classes[i]: i for i in range(len(classes))}\n",
    "class_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21bd9361-fe3f-4f42-a6fd-46a1605505a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Classification Dataset: Using iterator.\n",
    "# In the secind case using ___getitem__ is used for reading each imagemr\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "class MiceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_set, class_labels, class_to_int, transforms = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.data_set = data_set\n",
    "        self.class_to_int = class_to_int\n",
    "        self.class_labels = class_labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image= Image.open(self.data_set[index])\n",
    "        #croping image\n",
    "        image = np.asarray(image)[150:850, 250:950]\n",
    "        \n",
    "        label = self.class_labels[index]\n",
    "        label = self.class_to_int[label]\n",
    "        \n",
    "        \n",
    "        #Applying transforms on image\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        #return image, label\n",
    "        return image, label\n",
    "        \n",
    "    #its not important though   \n",
    "    def __len__(self):\n",
    "        #print (len(self.imgs_list))\n",
    "        return (len(self.data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62803052-dac9-41fa-8012-d0f0cb342981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms\n",
    "train_dataset = MiceDataset(X_train, y_train, class_to_int, transforms=get_train_transform())\n",
    "test_dataset = MiceDataset(X_test, y_test, class_to_int, transforms=get_test_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ffab3f-cec0-4e07-b141-05ef81df54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, RandomSampler\n",
    "#Data Loader  -  using Sampler \n",
    "train_random_sampler = RandomSampler(train_dataset)\n",
    "test_random_sampler = RandomSampler(test_dataset)\n",
    "\n",
    "# dataloader\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, sampler=train_random_sampler, num_workers=0)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=16, sampler=test_random_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc668df-f95f-4763-819b-18add2835e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([2, 4, 2, 0, 6, 5, 2, 5, 6, 0, 7, 5, 6, 5, 4, 2])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd650147-5a0b-42f6-b2a1-319d7f6835c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2bc96b-a303-4dc2-b404-1f39e4d94ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c68a2-709d-410d-8a8a-dd33ffdf5df7",
   "metadata": {},
   "source": [
    "# Transfering data to the device in use (In our case GPU)\n",
    "train_dl = DeviceDataLoader(train_data_loader, device)\n",
    "valid_dl = DeviceDataLoader(test_data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a22abf0-313b-4647-979a-6aef3694a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train':train_data_loader, 'test':test_data_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c777f70-0139-486e-875d-556f7747d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {'train':len(X_train), 'test':len(X_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5dec80a-cee6-46c7-b640-e5860d1af2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf2379-3cdf-4bcb-9233-12b27dd54a9a",
   "metadata": {},
   "source": [
    "next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a2e88-ce25-45a8-a309-b2474ae8a0a3",
   "metadata": {},
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60ad809b-38ab-4d30-8303-3495751b184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    result = {'train_acc':[], 'train_loss':[], 'valid_acc':[], 'valid_loss':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                #print('training...')\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                #print('dataloader phase: training')\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #outputs = model(inputs).softmax(dim=1)\n",
    "                    \n",
    "                    '''printing '''\n",
    "                    #print(f'outputs: {outputs}')\n",
    "                    '''trying out the softmax'''\n",
    "                    #print(f'softmax: {outputs.softmax(dim=1)}')\n",
    "                    \n",
    "                    pred_values, preds_indc = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    '''printing '''\n",
    "                   # print(f'loss: {loss}')\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds_indc == labels.data)\n",
    "                '''can be done something here for calculating accuracy\n",
    "                but I donot know how to use the prediction value to calculate the accuracy'''\n",
    "                \n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            #print(outputs) # wanna see the outputs\n",
    "            \n",
    "            if phase =='train':\n",
    "                result['train_acc'].append(epoch_acc)\n",
    "                result['train_loss'].append(epoch_loss)\n",
    "            else:\n",
    "                result['valid_acc'].append(epoch_acc)\n",
    "                result['valid_loss'].append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    #print(result)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac7afb5b-bb50-405c-8877-0d10d35f8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizr and model prediction\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #outputs = model(inputs).softmax(dim=1)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]) + class_names[labels[j]])\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d74d901-d518-4ba2-ba87-6201128ef1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet152(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63c988ec-5b76-40e6-b300-bd0957852918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "num_ftrs = model_ft.fc.in_features\n",
    "print(num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89a18db9-3143-4c51-b5b7-9baa675f5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ft.fc = nn.Linear(num_ftrs, 6, bias=True)\n",
    "num_classes = len(labels.unique())\n",
    "model_ft.fc = nn.Sequential(nn.Linear(2048, num_classes))\n",
    "\n",
    "model_ft = model_ft.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1fbc27-d45a-449c-9212-aa4ffce4594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8af263-e45f-4469-9bf0-ae0bd3e0ad7c",
   "metadata": {},
   "source": [
    "##claculating weights for each class\n",
    "samples = []\n",
    "samples = torch.tensor([len(all_files[keys]) for keys in all_files.keys()])\n",
    "sam_weights = samples/torch.sum(samples)\n",
    "inv_samples = 1/sam_weights\n",
    "loss_weight = inv_samples/torch.sum(inv_samples)\n",
    "loss_weight = loss_weight.to(device)\n",
    "loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a70f357-7480-4e66-89b5-5b04891ea870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:20<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.0917 Acc: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7998 Acc: 0.1111\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9999 Acc: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6705 Acc: 0.4333\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8783 Acc: 0.2381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6944 Acc: 0.4333\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.8678 Acc: 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.8578 Acc: 0.1667\n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7751 Acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7423 Acc: 0.4333\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.7785 Acc: 0.3238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7532 Acc: 0.4333\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6710 Acc: 0.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6713 Acc: 0.4333\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6473 Acc: 0.3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7282 Acc: 0.4333\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5765 Acc: 0.3810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7496 Acc: 0.4333\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.6041 Acc: 0.3810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6307 Acc: 0.4333\n",
      "Training complete in 3m 23s\n",
      "Best val Acc: 0.433333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train and evaluate\n",
    "#wts = torch.tensor([0.48, 0.15, 0.11, 0.2, 0.06]).to(device)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight=loss_weight)3 with weighted loss\n",
    "criterion = nn.CrossEntropyLoss()  # without weighted loss\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.9)\n",
    "num_epochs=10\n",
    "model_ft, result = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06ab3888-795e-4fc9-856d-91af8313240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = [x.cpu() for x in result['train_acc']]\n",
    "test_accuracy = [x.cpu() for x in result['valid_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1250ae35-a30b-49ce-91dc-a843ae8e5803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plotting the performace\n",
    "fig, ax = plt.subplots()\n",
    "train_acc,  = ax.plot(train_accuracy, label='train accuracy')\n",
    "valid_acc,  = ax.plot(test_accuracy, label='validation accuracy')\n",
    "\n",
    "#l1,  = ax.plot([1,2,3], label=\"Line 1\")\n",
    "#l2, = ax.plot([2,3,4], label=\"Line 2\")\n",
    "ax.legend(handles=[train_acc, valid_acc], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c868b5e-a660-42fb-b75d-7da3e6b59c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0ea99-e28f-4cb3-82d7-a61dff2b3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "I need to prepare proper dataset\n",
    "\n",
    "Need to prepare the test script to produce csv result from the video\n",
    "then work on the visual data generation\n",
    "implementation of kmeans clusturing and differnt visualization method for showing proper data analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
